gfe-db
======

Graph database representing IPD-IMGT/HLA sequence data as GFE.

<!-- Need to update image -->
<!-- ![GFE Schema](img/gfe-schema.png) -->

## Table of Contents
- [gfe-db](#gfe-db)
  - [Table of Contents](#table-of-contents)
  - [Project Structure](#project-structure)
  - [Description](#description)
    - [Build Service](#build-service)
    - [Load Service](#load-service)
    - [Database Service](#database-service)
    - [CloudFormation Templates](#cloudformation-templates)
  - [Installation](#installation)
    - [Prerequisites](#prerequisites)
  - [Usage](#usage)
    - [Deployment using Makefile](#deployment-using-makefile)
  - [Local Development](#local-development)
    - [Creating a Python Virtual Environment](#creating-a-python-virtual-environment)
    - [Environment Variables](#environment-variables)
    - [Run Neo4j Docker](#run-neo4j-docker)
    - [Load the dataset into Neo4j](#load-the-dataset-into-neo4j)
    - [Memory Management](#memory-management)
  - [Clean Up](#clean-up)
  - [Authors](#authors)
  - [References & Links](#references--links)

## Project Structure
```
.
├── LICENSE
├── Makefile
├── README.md
├── build                         # Build service for AWS Batch
│   ├── Dockerfile
│   ├── (logs)
│   ├── requirements.txt
│   ├── run.sh
│   ├── scripts
│   │   └── get_alignments.sh
│   └── src
│       ├── build_gfedb.py
│       └── constants.py
├── cfn
│   ├── database-stack.yml
│   ├── master-stack.yml          # Master CloudFormation template
│   ├── setup.yml                 # Setup template 
│   └── update-pipeline-stack.yml
├── (data)                        # Data directory used for local builds
│   ├── (3430)
│   │   ├── (csv)
│   │   └── (hla.3430.dat)
│   └── csv
├── deploy.sh                     # Deploy script
├── load                          # Load service for AWS Batch
│   ├── Dockerfile
│   ├── cypher
│   ├── requirements.txt
│   ├── run.sh
│   └── src
│       └── load_gfedb.py
└── neo4j                         # Database service
    ├── Dockerfile
    ├── (logs)
    └── plugins
```

## Description
The `gfe-db` represents IPD-IMGT/HLA sequence data as GFE nodes and relationships in a Neo4j graph database. The architecture to run and update `gfe-db` contains 3 basic components:
- Build Service
- Load Service
- Database Service

This project is meant to be deployed and run on AWS.

### Build Service
The build service is triggered when a new IMGT/HLA version is released. AWS Batch is used to deploy a container to an EC2 instance which will run the build script and generate a dataset of CSVs. These are uploaded to S3 where they can be accessed by the load service. This service is located inside the `build/` directory.

### Load Service
The load service runs once the build service completes. For each CSV file generated by the build service, a pre-signed URL is created and inserted into the `LOAD CSV FROM ...` statement within the Cypher script. Each statement in the script is sent to the Neo4j server using the HTTP API. The load service runs until Neo4j is done loading. This service is located inside the `load/` directory.

### Database Service
Neo4j is deployed within a Docker container to an EC2 instance. Indexes and constraints are set to expedite transactions. The browser can be accessed at port 7474 of the public DNS server found in the EC2 console. This service is located inside the `neo4j/` directory.

### CloudFormation Templates
CloudFormation templates define the architecture that is deployed to AWS. The basic resources include:
- VPC with public subnets
- S3 bucket for templates, data, backups and logs
- IAM permissions
- AWS Batch job definitions, queues and compute environments for build and load services
- StepFunctions state machine to orchestrate the build and load service
- ECR repositories to host the container images used for the build and load services
- EC2 Launch Template for deploying Neo4j

```bash
.
└── cfn
    ├── database-stack.yml        # Provisions a VPC and database to EC2
    ├── master-stack.yml          # Deploys the database and update pipeline stacks
    ├── setup.yml                 # Provisions the S3 bucket used for template artifacts, secrets, data and logs
    └── update-pipeline-stack.yml # Provisions the StepFunctions workflow and AWS Batch resources
```

<!-- ## To Do's
- [ ] Use Fargate with AWS Batch for the load service instead of EC2 to save cost
- [x] Create nested cloudformation templates
- [ ] Add CI/CD for Docker images
- [ ] Add trigger for when a new IMGT/HLA version is released
- [x] Load script can be optimized
  - [x] Better logging
  - [x] Clean up the Cypher script to avoid Neo4j errors
- [ ] Add a Makefile
- [ ] Add SSL policy for Neo4j to use HTTPS
- [ ] Deploy Neo4j with the APOC and Data Science plugins
- [ ] Update the userdata in the database.yml template to use the current version
- [ ] Add structured logs to the scripts
- [ ] Remove logs, CSVs from S3 prefix before a new build begins
- [ ] Update the Neo4j configuration to set users and roles for security
- [ ] Add architecture diagram to documentation
- [ ] Add constraints to cfn parameters
- [ ] Add logic to allow building and loading locally -->

## Installation
Follow the steps to set up a local development environment.

### Prerequisites
* Python 3.8
* GNU Make 3.81
* Docker
* AWS CLI
* jq

## Usage

### Deployment using Makefile
Make sure to update your AWS credentials in `~/.aws/credentials`. 

```bash
# Get an overview of make arguments and variables
make

# Deploy infrastructure to AWS
make deploy

# Build and load 1000 alleles from IMGT/HLA release version 3450
# Leave limit blank to load all alleles
make run release=3450 limit=1000

# Delete all data and tear down the architecture
make delete
```

## Local Development

### Creating a Python Virtual Environment
When developing locally, you will need to create individual virtual environments inside the `build/` and `load/` directories, since they require different dependencies:
```bash
# Build service
cd build
python3 -m venv ./build/.venv-build
source ./build/.venv-build/bin/activate
pip install -U pip
pip install -r requirements.txt

# Load service
cd load
python3 -m venv ./load/.venv-load
source ./load/.venv-load/bin/activate
pip install -U pip
pip install -r requirements.txt
```

### Environment Variables
Add a `.env` file with the following variables.
```bash
GFE_BUCKET=<value>
RELEASES=<value>
ALIGN=<value>
KIR=<value>
MEM_PROFILE=<value>
LIMIT=<value>
NEO4J_HOST=<value>
NEO4J_USERNAME=<value>
NEO4J_PASSWORD=<value>
```
Run the command to export environment variables to the shell.
```bash
set -a
source .env
set +a
```

*Important:* *Always use a `.env` file or AWS SSM Parameter Store or Secrets Manager for sensitive variables like credentials and API keys. Never hard-code them, including when developing. AWS will quarantine an account if any credentials get accidentally exposed and this will cause problems. **MAKE SURE `.env` IS LISTED IN `.gitignore`.**

<!-- ## Usage
Follow these steps in sequence to build and load `gfe-db` locally. Make sure that your environment variables are set correctly before proceeding. -->

### Run Neo4j Docker
Build the Docker image as defined in the Dockerfile. See [Configuring Neo4j in Dockerfile](#Configuring-Neo4j-in-Dockerfile) for important configuration settings.
```
cd neo4j
docker build --tag gfe-db .
```
Run the container to start Neo4j in Docker.
```
# Run container to start Neo4j
docker run -d --name gfe-db \
  -v "$(pwd)"/../data/csv/:/var/lib/neo4j/import \
  -v "$(pwd)"/../neo4j/plugins:/var/lib/neo4j/plugins \
  -v "$(pwd)"/../neo4j/logs:/var/lib/neo4j/logs \
  -p 7474:7474 -p 7473:7473 \
  -p 7687:7687 gfe-db
```
Access the container logs during startup to check the status of Neo4j.
```bash
docker logs -f gfe-db
```
Stop and restart as needed.
```bash
# Stop container
docker stop gfe-db

# Start container
docker start gfe-db
```

<!-- ### Build GFE dataset
Run the command to build the container for the build service.
```
# Build and run Docker locally
cd build
docker build --tag gfe-db-build-service .
```
Run the command to start the build. (Requires an S3 bucket)
```
docker run \
  --rm \
  -v "$(pwd)"/../data:/opt/data \
  -v "$(pwd)"/logs:/opt/app/logs \
  -e GFE_BUCKET='<S3 bucket name>' \
  -e RELEASES='3440' \
  -e ALIGN='False' \
  -e KIR='False' \
  -e MEM_PROFILE='True' \
  -e LIMIT='100' \
  --name gfe-db-build-service \
  gfe-db-build-service:latest
``` 
```
# Load from Docker locally
cd load
docker build -t gfe-db-load-service .
docker run gfe-db-load-service:latest
```

### Load the dataset into Neo4j
Once the container is running, the Neo4j server is up, and the dataset has been created, run the command to load it into Neo4j.
```
bash bin/load_db.sh
``` -->

<!-- ### Running the GFE database in Neo4j 4.2 using Docker
This README outlines the steps for building and running a development version of `gfe-db` in a local Docker container. Docker will deploy an instance of Neo4j 4.2 including the [APOC](https://neo4j.com/labs/apoc/4.1/) and [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/) plugins. GFE data is stored in the `data/csv/` directory which is mounted as an external volume within the container when run. This keeps the data outside the container so that it can be updated easily. -->

<!-- ## Notebooks
To use the virtual environment inside Jupyter Notebook, first activate the virtual environment, then create a kernel for it.
```bash
# Install ipykernal
pip install ipykernel

# Add the kernel
python3 -m ipykernel install --user --name=<environment name>
``` -->

<!-- ### `1.0-load-gfe-db`
Python notebook for developing the load service using the Neo4j HTTP API, Requests and Boto3 libraries.

### `1.0-refactor-gfedb_utils`
Development notebook for refactoring `gfe-db` and the `build/src/build_gfedb.py` module used for building the CSV datasets. -->

<!-- ## Running Tests -->

## Configuring Neo4j in Dockerfile
Configuration settings for Neo4j are passed through environment variables in the Dockerfile.

### Username & Password
The username and password is set as follows:
```Dockerfile
# Dockerfile
ENV NEO4J_AUTH=neo4j/gfedb
```

### Memory Management
Optimal memory for Neo4j depends on available RAM. Loading and querying a larger dataset will require more memory allocated. Make sure that the Docker daemon is configured to handle whatever values are given here. For more information on memory management in Neo4j, see the [Neo4j Operations Manual](https://neo4j.com/docs/operations-manual/current/performance/memory-configuration/).
```Dockerfile
# Dockerfile; Rebuild the image after updating these
ENV NEO4J_dbms_memory_heap_initial__size=2G
ENV NEO4J_dbms_memory_heap_max__size=2G
```

<!-- ## Deployment
`gfe-db` is deployed using Docker to an EC2 instance. Automated builds and loading of `gfe-db` on AWS is orchestrated using AWS Batch and StepFunctions. The infrastructure is defined using CloudFormation templates.

1. Make sure to update your AWS credentials in `~/.aws/credentials`.
2. Deploy the CloudFormation stacks and set the default region to `us-east-1` (there are issues with S3 pre-signed URLs in the other regions).
   ```bash
   cd gfe-db
   bash deploy.sh
   ```
3. In the AWS ECR console, follow the instructions in each ECR repo to build, tag and push the images to that repo.
4. In the Neo4j browser, run the `load/cypher/create_index.cyp` script.
5. Trigger an update using StepFunctions by starting an execution with the following input:
   ```json
   {
     "params": {
       "environment": {
         "RELEASES": "3450",
         "ALIGN": "False",
         "KIR": "False",
         "MEM_PROFILE": "False",
         "LIMIT": ""
       }
     }
   }
   ```
  Update the parameters to whatever is desired. Leaving `LIMIT` blank will build the entire GFE dataset (~30,000 alleles).
6. Get the `Neo4jDatabaseEndpoint` parameter from the `database-stack.yml` and load it in the browser on port 7474:
   ```bash
   # Example
   18.215.230.187:7474
   ```
   The graph should be loaded once the StepFunctions completes.


## Clean Up
To delete a stack and it's resources, use the CloudFormation console or run the command. S3 buckets and ECR repositories must be empty before they can be deleted.
```bash
aws cloudformation delete-stack --stack-name <stack name>
``` -->

<!-- ### Local Clean-up
Delete the Docker container.
```bash
docker stop gfe
docker rm gfe
```
Delete the Docker image.
```bash
# List the images and get the IMAGE IDs for gfe-db and neo4j
docker image ls

# Remove the images for gfe-db:latest and neo4j:4.2 by the IMAGE ID
docker image rm <IMAGE ID> <IMAGE ID>
```
To delete the Jupyter Notebook kernel, run the command.
```bash
jupyter kernelspec uninstall gfe-db
``` -->

## Troubleshooting
* Check your AWS credentials in `~/.aws/credentials`
* Check that the environment variables have been exported
* Sometimes Neo4j sets permissions on mounted volumes. To get around this run this from the project root:
  ```bash
  sudo chmod -R 777 .
  ```
* Check that the virtual environment is activated: `source .venv/bin/activate`
* Check that requirements are installed: `pip install -r requirements.txt`
* Check that Python 3.8 is being used

## Authors
**Primary Contact:** Gregory Lindsey ([@abk7777](https://github.com/abk7777))

## References & Links
 * [hub.docker.com/r/nmdpbioinformatics/service-gfe-submission](https://hub.docker.com/r/nmdpbioinformatics/service-gfe-submission)
 * [service-gfe-submission.readthedocs.io](https://service-gfe-submission.readthedocs.io/en/latest/index.html)
 * [github.com/nmdp-bioinformatics/service-feature](https://github.com/nmdp-bioinformatics/service-feature)
 * [github.com/nmdp-bioinformatics/HSA](https://github.com/nmdp-bioinformatics/HSA)
 * [bioinformatics.bethematchclinical.org](https://bioinformatics.bethematchclinical.org)
 * [feature.nmdp-bioinformatics.org](https://feature.nmdp-bioinformatics.org)
 * [gfe.b12x.org](http://gfe.b12x.org)

-----------------
<br>
<p align="center">
  <img src="https://bethematch.org/content/site/images/btm_logo.png">
</p>