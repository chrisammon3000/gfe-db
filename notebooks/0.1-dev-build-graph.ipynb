{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gfe-db / Build graph\n",
    "\n",
    "This notebook contains scripts for an update pipeline to [`nmdp-bioinformatics/gfe-db`](https://github.com/nmdp-bioinformatics/gfe-db). When run the notebook creates flat CSV files that can be read and parsed by Cypher's `LOAD CSV` clause to either create new nodes and relationships or merge existing ones.\n",
    "\n",
    "#### Key Columns\n",
    "Relationships are defined by a key columns in each CSV:\n",
    "- `(:GFE)-[:HAS_FEATURE]->(:FEATURE)` on `hla_name` (or `alleleId`)\n",
    "- `(:GFE)-[:HAS_SEQUENCE]->(:SEQUENCE)` on `sequence`\n",
    "- `(:GFE)-[:HAS_ALIGNMENT]->(:SEQUENCE)` on `sequence`\n",
    "- `(:GFE)-[:HAS_ALIGNMENT]->(:GEN_ALIGN)`, `(:NUC_ALIGN)`, `(:PROT_ALIGN)` on `a_name`\n",
    "- `(:SEQUENCE)-[:HAS_CDS]->(:CDS)` on `alleleId`\n",
    "- `(:IMGT_HLA)-[:HAS_GFE]->(:GFE)` on `hla_name`\n",
    "- `(:IMGT_HLA)-[:HAS_FEATURE]->(:FEATURE)` on `hla_name`\n",
    "- `(:IMGT_HLA)-[:HAS_SEQUENCE]->(:SEQUENCE)` on `alleleId`\n",
    "- `(:IMGT_HLA)-[:HAS_ALIGNMENT]->(:SEQUENCE)` on `alleleId`\n",
    "- `(:IMGT_HLA)-[:HAS_ALIGNMENT]->(:GEN_ALIGN)`, `(:NUC_ALIGN)`, `(:PROT_ALIGN)` on `hla_name`\n",
    "- `(:IMGT_HLA)<-[:G]-(:G)` on `hla_name`\n",
    "- `(:IMGT_HLA)<-[:lg]-(:lg)` on `hla_name`\n",
    "- `(:IMGT_HLA)<-[:lgx]-(:lgx)` on `hla_name`\n",
    "- `(:IMGT_HLA)<-[:2nd_FIELD]-(:2nd_FIELD)` on `hla_name`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path[0] = '../'\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import ast\n",
    "\n",
    "from bin.build_gfedb import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(name)-35s - %(levelname)-5s - %(funcName)s %(lineno)d: - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %I:%M:%S %p',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgt_hla = 'https://www.ebi.ac.uk/ipd/imgt/hla/docs/release.html'\n",
    "imgt_hla_media_url = 'https://media.githubusercontent.com/media/ANHIG/IMGTHLA/'\n",
    "imgt_hla_raw_url = 'https://raw.githubusercontent.com/ANHIG/IMGTHLA/'\n",
    "\n",
    "imgt_kir = 'https://www.ebi.ac.uk/ipd/kir/docs/version.html'\n",
    "kir_url = 'ftp://ftp.ebi.ac.uk/pub/databases/ipd/kir/KIR.dat'\n",
    "\n",
    "\n",
    "data_dir = \"../data/\" #os.path.dirname(__file__) + \"/../../data/\"\n",
    "\n",
    "expre_chars = ['N', 'Q', 'L', 'S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastseqid = 1\n",
    "lastid = 1\n",
    "lastcdsid = 1\n",
    "\n",
    "seqids = {}\n",
    "cdsids = {}\n",
    "alleleids = {}\n",
    "group_edges = {}\n",
    "trans_edges = {}\n",
    "\n",
    "# The alleles are removed when the allele_nodes.csv is built\n",
    "skip_alleles = [\"HLA-DRB5*01:11\", \"HLA-DRB5*01:12\", \"HLA-DRB5*01:13\",\n",
    "                \"HLA-DRB5*02:03\", \"HLA-DRB5*02:04\", \"HLA-DRB5*02:05\",\n",
    "                \"HLA-DRB5*01:01:02\", \"HLA-DRB5*01:03\", \"HLA-DRB5*01:05\",\n",
    "                \"HLA-DRB5*01:06\", \"HLA-DRB5*01:07\", \"HLA-DRB5*01:09\",\n",
    "                \"HLA-DRB5*01:10N\", \"HLA-C*05:208N\", \"HLA-C*05:206\"]\n",
    "\n",
    "hla_loci = ['HLA-A', 'HLA-B', 'HLA-C', 'HLA-DRB1', 'HLA-DQB1',\n",
    "            'HLA-DPB1', 'HLA-DQA1', 'HLA-DPA1', 'HLA-DRB3',\n",
    "            'HLA-DRB4', 'HLA-DRB5']\n",
    "\n",
    "hla_align = ['HLA-A', 'HLA-B', 'HLA-C', 'HLA-DRB1', 'HLA-DQB1',\n",
    "             'HLA-DPB1', 'HLA-DQA1', 'HLA-DPA1']\n",
    "\n",
    "kir_loci = [\"KIR3DS1\", \"KIR3DP1\", \"KIR3DL3\", \"KIR3DL2\", \"KIR3DL1\",\n",
    "            \"KIR2DS5\", \"KIR2DS4\", \"KIR2DS3\", \"KIR2DS2\", \"KIR2DS1\",\n",
    "            \"KIR2DP1\", \"KIR2DL5B\", \"KIR2DL5A\", \"KIR2DL4\"]\n",
    "\n",
    "kir_aligloci = [\"KIR2DL4\", \"KIR2DP1\", \"KIR2DS1\", \"KIR2DS2\", \"KIR2DS3\",\n",
    "                \"KIR2DS4\", \"KIR2DS5\", \"KIR3DL1\", \"KIR3DL2\", \"KIR3DL3\",\n",
    "                \"KIR3DP1\"]\n",
    "\n",
    "ard_groups = ['G', 'lg', 'lgx']\n",
    "\n",
    "align = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kir = None\n",
    "\n",
    "if kir:\n",
    "    load_loci = hla_loci + kir_loci\n",
    "else:\n",
    "    load_loci = hla_loci\n",
    "\n",
    "from seqann import gfe\n",
    "gfe_maker = gfe.GFE(verbose=True, verbosity=2,\n",
    "                load_features=False, store_features=True,\n",
    "                loci=load_loci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbversions = [\"3360\"]\n",
    "\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '' #args.out_dir\n",
    "release_n = 1 #args.number\n",
    "releases = '3360'#args.releases\n",
    "verbosity = 1\n",
    "\n",
    "align = True\n",
    "kir = True\n",
    "debug = False\n",
    "verbose = True\n",
    "\n",
    "#if args.kir:\n",
    "kir = True\n",
    "\n",
    "#if args.align:\n",
    "align = True\n",
    "\n",
    "#if args.verbose:\n",
    "verbose = True\n",
    "\n",
    "if kir:\n",
    "    load_loci = hla_loci + kir_loci\n",
    "else:\n",
    "    load_loci = hla_loci\n",
    "\n",
    "#if args.debug:\n",
    "#    logging.info(\"Running in debug mode\")\n",
    "#    load_loci = [\"HLA-A\"]\n",
    "#    kir = False\n",
    "#    debug = True\n",
    "#    verbose = True\n",
    "#    verbosity = 2\n",
    "#    release_n = 1\n",
    "\n",
    "gfe_e = []\n",
    "seq_e = []\n",
    "seq_n = []\n",
    "cds_n = []\n",
    "grp_e = []\n",
    "trs_e = []\n",
    "allele_n = []\n",
    "\n",
    "# Get last five IMGT/HLA releases\n",
    "if releases:\n",
    "    dbversions = [db for db in releases.split(\",\")]\n",
    "else:\n",
    "    dbversions = pd.read_html(imgt_hla)[0]['Release'][0:release_n].tolist()\n",
    "\n",
    "# Get latest IMGT/KIR release\n",
    "kir_release = pd.read_html(imgt_kir)[0]['Release'][0]\n",
    "\n",
    "from seqann import gfe\n",
    "gfe_maker = gfe.GFE(verbose=verbose, verbosity=verbosity,\n",
    "                    load_features=False, store_features=True,\n",
    "                    loci=load_loci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cds(allele):\n",
    "\n",
    "    feat_types = [f.type for f in allele.features]\n",
    "\n",
    "    if \"CDS\" in feat_types:\n",
    "        cds_features = allele.features[feat_types.index(\"CDS\")]\n",
    "        if 'translation' in cds_features.qualifiers:\n",
    "\n",
    "            if cds_features.location is None:\n",
    "                logger.info(\"No CDS location for feature in allele: \", allele.name)\n",
    "            else:\n",
    "                bp_seq = str(cds_features.extract(allele.seq))\n",
    "                aa_seq = cds_features.qualifiers['translation'][0]\n",
    "                \n",
    "    return bp_seq, aa_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the datasets for the HLA graph \n",
    "def build_hla_graph(dbversions, to_csv=False, limit=None):\n",
    "    \n",
    "    # Loop through DB versions\n",
    "    for dbversion in dbversions:\n",
    "\n",
    "        imgt_release = f'{dbversion[0]}.{dbversion[1:3]}.{dbversion[3]}'\n",
    "\n",
    "        db_striped = ''.join(dbversion.split(\".\"))\n",
    "\n",
    "        if align:\n",
    "            gen_aln, nuc_aln, prot_aln = hla_alignments(db_striped)\n",
    "\n",
    "        ard = ARD(db_striped)\n",
    "\n",
    "        # The github URL changed from 3350 to media\n",
    "        if int(db_striped) < 3350:\n",
    "            dat_url = imgt_hla_raw_url + db_striped + '/hla.dat'\n",
    "        else:\n",
    "            dat_url = imgt_hla_media_url + db_striped + '/hla.dat'\n",
    "\n",
    "        dat_file = data_dir + 'hla.' + db_striped + \".dat\"\n",
    "\n",
    "        # Downloading DAT file\n",
    "        if not os.path.isfile(dat_file):\n",
    "            if verbose:\n",
    "                logging.info(\"Downloading dat file from \" + dat_url)\n",
    "            urllib.request.urlretrieve(dat_url, dat_file)\n",
    "\n",
    "        # Parse DAT file\n",
    "        a_gen = SeqIO.parse(dat_file, \"imgt\")\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(\"Finished parsing dat file\")\n",
    "\n",
    "        def _build_csv_files(a_gen):\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            ### Initialize lists for CSV output (input to LOAD CSV in Neo4j)\n",
    "            # Lists contain unique dicts and are converted to dataframes, then output to CSV for Neo4j import\n",
    "            gfe_sequences = []\n",
    "            gen_alignments = []\n",
    "            nuc_alignments = []\n",
    "            prot_alignments = []\n",
    "            all_features = []\n",
    "            all_groups = []\n",
    "            all_cds = []\n",
    "            ###\n",
    "\n",
    "            for idx, allele in enumerate(a_gen):\n",
    "\n",
    "                if hasattr(allele, 'seq'):\n",
    "                    hla_name = allele.description.split(\",\")[0]\n",
    "                    loc = allele.description.split(\",\")[0].split(\"*\")[0]\n",
    "\n",
    "                    if hla_name in skip_alleles:\n",
    "                        logging.info(\n",
    "                            \"SKIPPING = \" + allele.description.split(\",\")[0] + \" \" + dbversion)\n",
    "                        continue\n",
    "\n",
    "                    if debug and (loc != \"HLA-A\" and i > 20):\n",
    "                        continue\n",
    "\n",
    "                    if (loc in hla_loci or loc == \"DRB5\") and (len(str(allele.seq)) > 5):\n",
    "                        if debug:\n",
    "                            logging.info(\n",
    "                                \"HLA = \" + allele.description.split(\",\")[0] + \" \" + dbversion)\n",
    "\n",
    "                        a_name = allele.description.split(\",\")[0].split(\"-\")[1]\n",
    "                        groups = [[\"HLA-\" + ard.redux(a_name, grp), grp] if ard.redux(a_name, grp) != a_name else None for\n",
    "                                  grp in ard_groups]\n",
    "                        seco = [[to_second(a_name), \"2nd_FIELD\"]]\n",
    "                        groups = list(filter(None, groups)) + seco\n",
    "                        complete_annotation = get_features(allele)\n",
    "                        ann = Annotation(annotation=complete_annotation,\n",
    "                                         method='match',\n",
    "                                         complete_annotation=True)\n",
    "\n",
    "                        # This process takes a long time\n",
    "                        features, gfe = gfe_maker.get_gfe(ann, loc)\n",
    "\n",
    "                        # gen_aln, nuc_aln, prot_aln\n",
    "                        aligned_gen = ''\n",
    "                        aligned_nuc = ''\n",
    "                        aligned_prot = ''\n",
    "\n",
    "                        if align:\n",
    "                            if allele.description.split(\",\")[0] in gen_aln[loc]:\n",
    "                                aligned_gen = gen_aln[loc][allele.description.split(\",\")[\n",
    "                                    0]]\n",
    "\n",
    "                            if allele.description.split(\",\")[0] in nuc_aln[loc]:\n",
    "                                aligned_nuc = nuc_aln[loc][allele.description.split(\",\")[\n",
    "                                    0]]\n",
    "\n",
    "                            if allele.description.split(\",\")[0] in prot_aln[loc]:\n",
    "                                aligned_prot = prot_aln[loc][allele.description.split(\",\")[\n",
    "                                    0]]\n",
    "\n",
    "                    ### Build dicts describing nodes and edges for each allele\n",
    "\n",
    "                    # Notes:\n",
    "                    # all edges are joined using foreign keys:\n",
    "                    #  - GFE --> SEQUENCE on alleleId\n",
    "                    #  - GFE --> [GEN_ALIGN, NUC_ALIGN, PROT_ALIGN] on a_name\n",
    "                    #  - GFE --> FEATURE on alleleId (or hla_name)\n",
    "                    # \"alleleId\" is assigned allele.id value\n",
    "                    # \"sequenceId\" is replaced with alleleId\n",
    "                    # feature key \"name\" is now \"term\"\n",
    "\n",
    "                    # Questions:\n",
    "                    # - Should GFE name be assigned to each node?\n",
    "\n",
    "                    # Separate CSV file\n",
    "                    gfe_sequence = {\n",
    "                        \"alleleId\": allele.id,\n",
    "                        \"gfe_name\": gfe,\n",
    "                        \"locus\": loc,\n",
    "                        \"hla_name\": hla_name,\n",
    "                        \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "                        \"sequence\": str(allele.seq),\n",
    "                        \"length\": len(str(allele.seq)),\n",
    "                        \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "                    # Separate CSV file, GFE foreign key: a_name\n",
    "                    gen_alignment = {\n",
    "                        \"label\": \"GEN_ALIGN\",\n",
    "                        \"hla_name\": hla_name,\n",
    "                        \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "                        \"length\": len(aligned_gen),\n",
    "                        \"rank\": \"0\", # TO DO: confirm how this value is derived\n",
    "                        \"bp_sequence\": aligned_gen,\n",
    "                        \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "                    # Separate CSV file, GFE foreign key: a_name\n",
    "                    nuc_alignment = {\n",
    "                        \"label\": \"NUC_ALIGN\",\n",
    "                        \"hla_name\": hla_name,\n",
    "                        \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "                        \"length\": len(aligned_nuc),\n",
    "                        \"rank\": \"0\", # TO DO: confirm how this value is derived\n",
    "                        \"bp_sequence\": aligned_nuc,\n",
    "                        \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "                    # Separate CSV file, GFE foreign key: a_name\n",
    "                    prot_alignment = {\n",
    "                        \"label\": \"PROT_ALIGN\",\n",
    "                        \"hla_name\": hla_name,\n",
    "                        \"a_name\": a_name, # hla_name.split(\"-\")[1]\n",
    "                        \"length\": len(aligned_prot),\n",
    "                        \"rank\": \"0\", # TO DO: confirm how this value is derived\n",
    "                        \"aa_sequence\": aligned_prot,\n",
    "                        \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "\n",
    "                    # Separate CSV file, GFE foreign key: alleleId\n",
    "                    allele_groups = []\n",
    "\n",
    "                    for group in groups:\n",
    "                        group_dict = {\n",
    "                            \"alleleId\": allele.id,\n",
    "                            \"hla_name\": hla_name,\n",
    "                            \"a_name\": a_name,\n",
    "                            \"ard_id\": group[0],\n",
    "                            \"ard_name\": group[1],\n",
    "                            \"locus\": loc,\n",
    "                            \"imgt_release\": imgt_release\n",
    "                        }\n",
    "\n",
    "                        allele_groups.append(group_dict)\n",
    "\n",
    "                    # Build CDS dict for CSV export, foreign key: alleleId, hla_name\n",
    "                    bp_seq, aa_seq = get_cds(allele)\n",
    "\n",
    "                    cds = {\n",
    "                        \"alleleId\": allele.id,\n",
    "                        \"hla_name\": hla_name,\n",
    "                        \"bp_sequence\": bp_seq,\n",
    "                        \"aa_sequence\": aa_seq,\n",
    "                        \"imgt_release\": imgt_release\n",
    "                    }\n",
    "\n",
    "                    # features preprocessing steps\n",
    "                    # 1) Convert seqann type to python dict using literal_eval\n",
    "                    # 2) add GFE foreign keys: alleleId, hla_name\n",
    "                    # 3) add columns: length\n",
    "\n",
    "                    # features contains list of seqann objects, converts to dict, destructive step\n",
    "                    features = [ast.literal_eval(str(feature).replace('\\'', '\"').replace('\\n', '')) for feature in features]\n",
    "\n",
    "                    # Append allele id's\n",
    "                    # Note: Some alleles may have the same feature, but it may not be the same rank, \n",
    "                    # so a feature should be identified with its allele by alleleId or HLA name\n",
    "                    for feature in features:\n",
    "                        feature[\"term\"] = feature[\"term\"].upper()\n",
    "                        feature[\"alleleId\"] = allele.id \n",
    "                        feature[\"hla_name\"] = hla_name\n",
    "                        feature[\"imgt_release\"] = imgt_release\n",
    "\n",
    "                        # Avoid null values in CSV for Neo4j import\n",
    "                        feature[\"hash_code\"] = \"none\" if not feature[\"hash_code\"] else feature[\"hash_code\"]\n",
    "                        #feature[\"hla_name\"] = \"none\" if not feature[\"hla_name\"] else feature[\"hla_name\"]\n",
    "\n",
    "                    # Append data to respective list\n",
    "                    data = zip(\n",
    "                        [gfe_sequences, gen_alignments, nuc_alignments, prot_alignments, all_cds],\n",
    "                        [gfe_sequence, gen_alignment, nuc_alignment, prot_alignment, cds]\n",
    "                    )\n",
    "\n",
    "                    for _list, _dict in data:\n",
    "                        _list.append(_dict)\n",
    "\n",
    "                    # Alignments, features, and ARD groups can all be concatenated since the keys are the same\n",
    "                    alignments = gen_alignments + nuc_alignments + prot_alignments\n",
    "                    all_features = all_features + features        \n",
    "                    all_groups = all_groups + allele_groups\n",
    "\n",
    "                # Break point for testing\n",
    "                if limit and idx == limit:\n",
    "                        break\n",
    "\n",
    "            csv_output = {\n",
    "                \"gfe_sequences\": gfe_sequences,\n",
    "                \"alignments\": alignments,\n",
    "                \"all_features\": all_features,\n",
    "                \"all_groups\": all_groups,\n",
    "                \"all_cds\": all_cds\n",
    "            }\n",
    "\n",
    "            return csv_output\n",
    "        \n",
    "        csv_output = _build_csv_files(a_gen)\n",
    "\n",
    "        if to_csv:\n",
    "            write_csv_files(csv_output, path, dbversion)\n",
    "            return csv_output\n",
    "        else:\n",
    "            return csv_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CSV files to local directory\n",
    "def write_csv_files(csv_output, path, dbversion):\n",
    "    \"\"\"Takes a dict of form \"csv_name\": data where csv_name is the CSV file to export\n",
    "    and data is a list of dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Output to CSV, include dbversion in name\n",
    "        for csv_name, data in csv_output.items():\n",
    "            dataframe = pd.DataFrame(data)\n",
    "            file_name = path + csv_name + f\".{dbversion}.csv\"\n",
    "            dataframe.to_csv(file_name, index=False)\n",
    "\n",
    "        logging.info(f'Saved CSV files to \"{path}\"')\n",
    "\n",
    "        return \n",
    "\n",
    "    except Exception as err:\n",
    "        logging.error(f'Failed to save CSV files to \"{path}\"')\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HLA process\n",
    "\n",
    "Code in this process contains variables that define nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbversions = [\"3410\", \"3420\", \"3430\"]\n",
    "dbversions = [\"3360\"]\n",
    "\n",
    "path = \"../data/csv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/09/2021 05:06:27 PM - root - INFO - Loading ../bin/../data/3360/A_gen.msf\n",
      "02/09/2021 05:06:27 PM - root - INFO - Loaded 1771 genomic HLA-A sequences\n",
      "02/09/2021 05:06:27 PM - root - INFO - Loading ../bin/../data/3360/A_nuc.msf\n",
      "02/09/2021 05:06:28 PM - root - INFO - Loaded 5016 nuc HLA-A sequences\n",
      "02/09/2021 05:06:28 PM - root - INFO - Loading ../bin/../data/3360/A_prot.msf\n",
      "02/09/2021 05:06:28 PM - root - INFO - Loaded 5016 prot HLA-A sequences\n",
      "02/09/2021 05:06:28 PM - root - INFO - Loading ../bin/../data/3360/B_gen.msf\n",
      "02/09/2021 05:06:29 PM - root - INFO - Loaded 2149 genomic HLA-B sequences\n",
      "02/09/2021 05:06:29 PM - root - INFO - Loading ../bin/../data/3360/B_nuc.msf\n",
      "02/09/2021 05:06:30 PM - root - INFO - Loaded 6094 nuc HLA-B sequences\n",
      "02/09/2021 05:06:30 PM - root - INFO - Loading ../bin/../data/3360/B_prot.msf\n",
      "02/09/2021 05:06:30 PM - root - INFO - Loaded 6094 prot HLA-B sequences\n",
      "02/09/2021 05:06:30 PM - root - INFO - Loading ../bin/../data/3360/C_gen.msf\n",
      "02/09/2021 05:06:31 PM - root - INFO - Loaded 2135 genomic HLA-C sequences\n",
      "02/09/2021 05:06:31 PM - root - INFO - Loading ../bin/../data/3360/C_nuc.msf\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loaded 4850 nuc HLA-C sequences\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loading ../bin/../data/3360/C_prot.msf\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loaded 4850 prot HLA-C sequences\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loading ../bin/../data/3360/DRB1_gen.msf\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loaded 94 genomic HLA-DRB1 sequences\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loading ../bin/../data/3360/DRB1_nuc.msf\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loaded 2479 nuc HLA-DRB1 sequences\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loading ../bin/../data/3360/DRB1_prot.msf\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loaded 2479 prot HLA-DRB1 sequences\n",
      "02/09/2021 05:06:32 PM - root - INFO - Loading ../bin/../data/3360/DQB1_gen.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 217 genomic HLA-DQB1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DQB1_nuc.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 1560 nuc HLA-DQB1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DQB1_prot.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 1560 prot HLA-DQB1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DPB1_gen.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 418 genomic HLA-DPB1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DPB1_nuc.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 1360 nuc HLA-DPB1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DPB1_prot.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 1360 prot HLA-DPB1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DQA1_gen.msf\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loaded 108 genomic HLA-DQA1 sequences\n",
      "02/09/2021 05:06:33 PM - root - INFO - Loading ../bin/../data/3360/DQA1_nuc.msf\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loaded 149 nuc HLA-DQA1 sequences\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loading ../bin/../data/3360/DQA1_prot.msf\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loaded 149 prot HLA-DQA1 sequences\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loading ../bin/../data/3360/DPA1_gen.msf\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loaded 67 genomic HLA-DPA1 sequences\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loading ../bin/../data/3360/DPA1_nuc.msf\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loaded 106 nuc HLA-DPA1 sequences\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loading ../bin/../data/3360/DPA1_prot.msf\n",
      "02/09/2021 05:06:34 PM - root - INFO - Loaded 106 prot HLA-DPA1 sequences\n",
      "02/09/2021 05:06:49 PM - root - INFO - Finished parsing dat file\n",
      "02/09/2021 05:06:49 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw2-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-4\n",
      "02/09/2021 05:06:49 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw3-1-1-1-2-1-1-1-1-1-1-1-1-1-1-1-3\n",
      "02/09/2021 05:06:49 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw2-1-2-1-1-1-1-1-1-1-1-1-1-1-1-1-19\n",
      "02/09/2021 05:06:49 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw46-1-1-1-1-1-90-1-1-1-1-1-1-1-1-1-18\n",
      "02/09/2021 05:06:50 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw333-1-1-1-280-1-1-1-1-1-1-1-1-1-1-1-4\n",
      "02/09/2021 05:06:50 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw333-1-1-1-1-1-1-1-1-1-219-1-1-1-1-1-6\n",
      "02/09/2021 05:06:50 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw333-1-1-1-279-1-1-1-1-1-1-1-1-1-1-1-4\n",
      "02/09/2021 05:06:50 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw363-1-1-1-1-1-308-1-1-1-1-1-1-1-1-1-119\n",
      "02/09/2021 05:06:50 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw1236-1-144-1-1-1-1-1-1-1-1-1-1-1-1-1-4\n",
      "02/09/2021 05:06:51 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw363-1-1-1-1-1-1-1-100-1-1-1-1-1-1-1-119\n",
      "02/09/2021 05:06:51 PM - Logger.seqann.gfe - INFO - GFE = HLA-Aw2-1-1-1-1-1-26-1-1-1-1-1-1-1-1-1-19\n",
      "02/09/2021 05:06:51 PM - root - INFO - Saved CSV files to \"../data/csv/\"\n"
     ]
    }
   ],
   "source": [
    "# Build the HLA graph under main()o\n",
    "csv_output = build_hla_graph(dbversions, write=True, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Validation\n",
    "\n",
    "Each list of dictionaries is exported as A CSV. The keys of each dictionary represent columns of the CSV output. If the column names are changed, the Cypher script must also be updated.\n",
    "\n",
    "Notes for testing:\n",
    "- For each dataframe output as CSV, check that there are no NULL values in the dataframes, they should be replaced with \"none\" for convenient loading in Cypher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gfe_sequences', 'alignments', 'all_features', 'all_groups', 'all_cds'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KIR\n",
    "\n",
    "Not working due to error:\n",
    "```python\n",
    "ValueError: Problem with 'CDS' feature:\n",
    "join(269..302,1267..1302,3751..4049,5579..5872,9027..9077,\n",
    "13337..13438,13901..13953,14052..14228)\n",
    "/codon_start=1\n",
    "/gene=\"KIR2DL1\"\n",
    "/allele=\"KIR2DL1*0450101N\"\n",
    "/product=\"KIR2DL1 Killer-cell Immunoglobulin-like Receptor\"\n",
    "/translation=\"MSLLFVSMACVGFFLLQGAWPHEGVHRNLPSWPTQVPWX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/30/2021 02:48:52 PM - root - INFO - Adding KIR to GFE DB\n",
      "01/30/2021 02:48:53 PM - root - INFO - Finished parsing KIR dat file\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Problem with 'CDS' feature:\njoin(269..302,1267..1302,3751..4049,5579..5872,9027..9077,\n13337..13438,13901..13953,14052..14228)\n/codon_start=1\n/gene=\"KIR2DL1\"\n/allele=\"KIR2DL1*0450101N\"\n/product=\"KIR2DL1 Killer-cell Immunoglobulin-like Receptor\"\n/translation=\"MSLLFVSMACVGFFLLQGAWPHEGVHRNLPSWPTQVPWX",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/GenBank/Scanner.py\u001b[0m in \u001b[0;36mparse_feature\u001b[0;34m(self, feature_key, lines)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0mvalue_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                             \u001b[0mvalue_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8b3a0ff8274f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mallele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkir_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallele\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/SeqIO/__init__.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(handle, format, alphabet)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown format '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# This imposes some overhead... wait until we drop Python 2.4 to fix it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/GenBank/Scanner.py\u001b[0m in \u001b[0;36mparse_records\u001b[0;34m(self, handle, do_features)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m# This is a generator function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/GenBank/Scanner.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, handle, do_features)\u001b[0m\n\u001b[1;32m    496\u001b[0m         )\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/GenBank/Scanner.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, handle, consumer, do_features)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;31m# Features (common to both EMBL and GenBank):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_feature_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/GenBank/Scanner.py\u001b[0m in \u001b[0;36mparse_features\u001b[0;34m(self, skip)\u001b[0m\n\u001b[1;32m   1145\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m                 feature_key, location, qualifiers = self.parse_feature(\n\u001b[0;32m-> 1147\u001b[0;31m                     \u001b[0mfeature_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m                 )\n\u001b[1;32m   1149\u001b[0m                 \u001b[0;31m# Try to handle known problems with IMGT locations here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/06 Upwork/Maiers/01 Projects/gfe-db/venv/lib/python3.7/site-packages/Bio/GenBank/Scanner.py\u001b[0m in \u001b[0;36mparse_feature\u001b[0;34m(self, feature_key, lines)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;31m# Bummer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             raise ValueError(\n\u001b[0;32m--> 366\u001b[0;31m                 \u001b[0;34m\"Problem with '%s' feature:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeature_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m             )\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Problem with 'CDS' feature:\njoin(269..302,1267..1302,3751..4049,5579..5872,9027..9077,\n13337..13438,13901..13953,14052..14228)\n/codon_start=1\n/gene=\"KIR2DL1\"\n/allele=\"KIR2DL1*0450101N\"\n/product=\"KIR2DL1 Killer-cell Immunoglobulin-like Receptor\"\n/translation=\"MSLLFVSMACVGFFLLQGAWPHEGVHRNLPSWPTQVPWX"
     ]
    }
   ],
   "source": [
    "# KIR process (broken)\n",
    "if kir:\n",
    "    if verbose:\n",
    "        logging.info(\"Adding KIR to GFE DB\")\n",
    "\n",
    "    kir_file = data_dir + 'KIR.dat'\n",
    "\n",
    "    if align:\n",
    "        aligned = kir_alignments()\n",
    "\n",
    "    # Downloading KIR\n",
    "    if not os.path.isfile(kir_file):\n",
    "        if verbose:\n",
    "            logging.info(\"Downloading KIR dat file from \" + kir_url)\n",
    "        urllib.request.urlretrieve(kir_url, kir_file)\n",
    "\n",
    "    kir_gen = SeqIO.parse(kir_file, \"imgt\")\n",
    "    if verbose:\n",
    "        logging.info(\"Finished parsing KIR dat file\")\n",
    "\n",
    "    i = 0\n",
    "    for idx, allele in enumerate(kir_gen):\n",
    "    \n",
    "        # Breakpoint for development testing\n",
    "        if idx == 1:\n",
    "                break\n",
    "        \n",
    "        if hasattr(allele, 'seq'):\n",
    "            loc = allele.description.split(\",\")[0].split(\"*\")[0]\n",
    "            if loc in kir_loci and len(str(allele.seq)) > 5:\n",
    "                if debug:\n",
    "                    logging.info(\n",
    "                        \"KIR = \" + allele.description.split(\",\")[0] + \" \" + kir_release)\n",
    "\n",
    "                groups = []\n",
    "                complete_annotation = get_features(allele)\n",
    "                ambigs = [\n",
    "                    a for a in complete_annotation if re.search(\"/\", a)]\n",
    "\n",
    "                aligned_seq = ''\n",
    "                if align:\n",
    "                    if allele.description.split(\",\")[0] in aligned[loc]:\n",
    "                        aligned_seq = aligned[loc][allele.description.split(\",\")[\n",
    "                            0]]\n",
    "\n",
    "                if ambigs:\n",
    "                    logging.info(\n",
    "                        \"AMBIGS \" + allele.description.split(\",\")[0] + \" \" + kir_release)\n",
    "                    annotations = []\n",
    "                    for ambig in ambigs:\n",
    "                        if debug:\n",
    "                            logging.info(\"AMBIG = \" + ambig)\n",
    "                        aterm = ambig.split(\"/\")[0].split(\"_\")[0]\n",
    "                        anno = {\n",
    "                            a: complete_annotation[a] for a in complete_annotation if a not in ambigs}\n",
    "                        anno.update(\n",
    "                            {ambig.split(\"/\")[0]: complete_annotation[ambig]})\n",
    "                        annotations.append(anno)\n",
    "\n",
    "                        anno2 = {\n",
    "                            a: complete_annotation[a] for a in complete_annotation if a not in ambigs}\n",
    "                        anno2.update(\n",
    "                            {aterm + \"_\" + ambig.split(\"/\")[1]: complete_annotation[ambig]})\n",
    "                        annotations.append(anno2)\n",
    "\n",
    "                    for annotation in annotations:\n",
    "                        ann = Annotation(annotation=annotation,\n",
    "                                         method='match',\n",
    "                                         complete_annotation=True)\n",
    "\n",
    "                        features, gfe = gfe_maker.get_gfe(ann, loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
